{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Activation Functions Exercise\n",
    "\n",
    "In this exercise, we will implement the forward and backward propagation of common activation functions . These functions are essential building blocks in neural networks and understanding their implementation will provide valuable insights into how neural networks operate.\n",
    "\n",
    "We will start by implementing the sigmoid and tanh activation functions, which are widely used in various neural network architectures. Each function will have a forward pass that computes the output and a backward pass that computes the gradients with respect to the inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from annp.layers import tanh_forward, tanh_backward,sigmoid_forward,sigmoid_backward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from annp.classifiers.fc_net import *\n",
    "from annp.gradient_check import eval_numerical_gradient_array\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0)\n",
    "\n",
    "\n",
    "def rel_error(x, y):\n",
    "    return np.max(np.abs(x - y) / (np.maximum(1e-8, np.abs(x) + np.abs(y))))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tanh activation: forward\n",
    "Implement the forward pass for the Tanh activation function `tanh_forward` in the `annp/layers.py` and test your implementation using the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing tanh_forward function:\n",
      "difference:  3.838674858587434e-09\n"
     ]
    }
   ],
   "source": [
    "# Test the tanh_forward function\n",
    "\n",
    "x = np.linspace(-1, 1, num=10).reshape(5, 2)\n",
    "\n",
    "out, _ = tanh_forward(x)\n",
    "correct_out = np.array([[-0.76159416, -0.65142936],\n",
    "                        [-0.5046724,  -0.32151274],\n",
    "                        [-0.11065611,  0.11065611],\n",
    "                        [ 0.32151274,  0.5046724 ],\n",
    "                        [ 0.65142936,  0.76159416]])\n",
    "\n",
    "# Compare your output with the direct calculation. The error should be on the order of 1e-8 or less\n",
    "print('Testing tanh_forward function:')\n",
    "print('difference: ', rel_error(out, correct_out))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tanh activation: backward\n",
    "Now implement the backward pass for the Tanh activation function in the `tanh_backward` function and test your implementation using numeric gradient checking:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing tanh_backward function:\n",
      "dx error:  1.0409584283255793e-10\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(231)\n",
    "x = np.random.randn(10, 10)\n",
    "dout = np.random.randn(*x.shape)\n",
    "\n",
    "dx_num = eval_numerical_gradient_array(lambda x: tanh_forward(x)[0], x, dout)\n",
    "\n",
    "_, cache = tanh_forward(x)\n",
    "dx = tanh_backward(dout, cache)\n",
    "\n",
    "# The error should be on the order of 1e-8 or less\n",
    "print('Testing tanh_backward function:')\n",
    "print('dx error: ', rel_error(dx_num, dx))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sigmoid activation: forward\n",
    "Implement the forward pass for the Sigmoid activation function in the `sigmoid_forward` function and test your implementation using the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing sigmoid_forward function:\n",
      "difference:  5.296529106595102e-09\n"
     ]
    }
   ],
   "source": [
    "# Test the sigmoid_forward function\n",
    "\n",
    "x = np.linspace(-1, 1, num=10).reshape(5, 2)\n",
    "\n",
    "out, _ = sigmoid_forward(x)\n",
    "correct_out = np.array([[0.26894142,0.31479902],\n",
    "                        [0.36457644, 0.41742979],\n",
    "                        [0.47225076, 0.52774924],\n",
    "                        [0.58257021, 0.63542356],\n",
    "                        [0.68520098, 0.73105858]])\n",
    "\n",
    "# Compare your output with the direct calculation. The error should be on the order of 1e-8 or less\n",
    "print('Testing sigmoid_forward function:')\n",
    "print('difference: ', rel_error(out, correct_out))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sigmoid activation: backward\n",
    "Now implement the backward pass for the Sigmoid activation function in the `sigmoid_backward` function and test your implementation using numeric gradient checking:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing sigmoid_backward function:\n",
      "dx error:  3.446520386706568e-11\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(231)\n",
    "x = np.random.randn(10, 10)\n",
    "dout = np.random.randn(*x.shape)\n",
    "\n",
    "dx_num = eval_numerical_gradient_array(lambda x: sigmoid_forward(x)[0], x, dout)\n",
    "\n",
    "_, cache = sigmoid_forward(x)\n",
    "dx = sigmoid_backward(dout, cache)\n",
    "\n",
    "# The error should be on the order of 1e-8 or less\n",
    "print('Testing sigmoid_backward function:')\n",
    "print('dx error: ', rel_error(dx_num, dx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
